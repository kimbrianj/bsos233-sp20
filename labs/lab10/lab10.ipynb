{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Classification\n",
    "\n",
    "This assignment covers **Chapter 17** from the textbook as well as lecture material from Weeks 12-13. Please complete this assignment by providing answers in cells after the question. Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**.\n",
    "\n",
    "This assignment is due by **11:59pm on Thursday, April 30**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datascience import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the classifiers in `sklearn` is very similar to finding the least squares regression line using `LinearRegression`. We first create the model object, then fit the model with our data. In our case, we will fit the data using our train data, and then predict for the test data (using the `predict_proba` method). \n",
    "\n",
    "This assignment will cover the main steps for applying machine learning models.\n",
    "- Create train and test sets\n",
    "- Fit the models using train set\n",
    "- Predict using the test set\n",
    "- Evaluate models using metrics such as precision and recall\n",
    "- Make your conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brazilian Sign Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brazilian Sign Language is a visual language used primarily by Brazilians who are deaf.  It is more commonly called Libras.  People who communicate with visual language are called *signers*.  Here is a video of someone signing in Libras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/mhIcuMZmyWM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f9ac0507860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo(\"mhIcuMZmyWM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs like Siri or Google Now begin the process of understanding human speech by classifying short clips of raw sound into basic categories called *phones*.  For example, the recorded sound of someone saying the word \"robot\" might be broken down into several phones: \"rrr\", \"oh\", \"buh\", \"aah\", and \"tuh\".  Phones are then grouped together into further categories like words (\"robot\") and sentences (\"I, for one, welcome our new robot overlords\") that carry more meaning.\n",
    "\n",
    "A visual language like Libras has an analogous structure.  Instead of phones, each word is made up of several *hand movements*.  As a first step in interpreting Libras, we can break down a video clip into small segments, each containing a single hand movement.  The task is then to figure out what hand movement each segment represents.\n",
    "\n",
    "We can do that with classification!\n",
    "\n",
    "The [data](https://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.names) in this exercise come from Dias, Peres, and Biscaro, researchers at the University of Sao Paulo in Brazil.  They identified 15 distinct hand movements in Libras (probably an oversimplification, but a useful one) and captured short videos of signers making those hand movements.  (You can read more about their work [here](http://ieeexplore.ieee.org/Xplore/login.jsp?url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F5161636%2F5178557%2F05178917.pdf&authDecision=-203). The paper is gated, so you will need to use the UMD Wi-Fi or VPN to access it.)\n",
    "\n",
    "For each video, they chose 45 still frames from the video and identified the location (in horizontal and vertical coordinates) of the signer's hand in each frame.  Since there are two coordinates for each frame, this gives us a total of 90 numbers summarizing how a hand moved in each video.  Those 90 numbers will be our *attributes*.\n",
    "\n",
    "Each video is *labeled* with the kind of hand movement the signer was making in it.  Each label is one of 15 strings like \"horizontal swing\" or \"vertical zigzag\".\n",
    "\n",
    "For simplicity, we're going to focus on distinguishing between just two kinds of movements: \"horizontal straight-line\" and \"vertical straight-line\".  We took the Sao Paulo researchers' original dataset, which was quite small, and used some simple techniques to create a much larger synthetic dataset.\n",
    "\n",
    "These data are in the file `movements.csv`.  Run the next cell to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Frame 1 x</th> <th>Frame 1 y</th> <th>Frame 2 x</th> <th>Frame 2 y</th> <th>Frame 3 x</th> <th>Frame 3 y</th> <th>Frame 4 x</th> <th>Frame 4 y</th> <th>Frame 5 x</th> <th>Frame 5 y</th> <th>Frame 6 x</th> <th>Frame 6 y</th> <th>Frame 7 x</th> <th>Frame 7 y</th> <th>Frame 8 x</th> <th>Frame 8 y</th> <th>Frame 9 x</th> <th>Frame 9 y</th> <th>Frame 10 x</th> <th>Frame 10 y</th> <th>Frame 11 x</th> <th>Frame 11 y</th> <th>Frame 12 x</th> <th>Frame 12 y</th> <th>Frame 13 x</th> <th>Frame 13 y</th> <th>Frame 14 x</th> <th>Frame 14 y</th> <th>Frame 15 x</th> <th>Frame 15 y</th> <th>Frame 16 x</th> <th>Frame 16 y</th> <th>Frame 17 x</th> <th>Frame 17 y</th> <th>Frame 18 x</th> <th>Frame 18 y</th> <th>Frame 19 x</th> <th>Frame 19 y</th> <th>Frame 20 x</th> <th>Frame 20 y</th> <th>Frame 21 x</th> <th>Frame 21 y</th> <th>Frame 22 x</th> <th>Frame 22 y</th> <th>Frame 23 x</th> <th>Frame 23 y</th> <th>Frame 24 x</th> <th>Frame 24 y</th> <th>Frame 25 x</th> <th>Frame 25 y</th> <th>Frame 26 x</th> <th>Frame 26 y</th> <th>Frame 27 x</th> <th>Frame 27 y</th> <th>Frame 28 x</th> <th>Frame 28 y</th> <th>Frame 29 x</th> <th>Frame 29 y</th> <th>Frame 30 x</th> <th>Frame 30 y</th> <th>Frame 31 x</th> <th>Frame 31 y</th> <th>Frame 32 x</th> <th>Frame 32 y</th> <th>Frame 33 x</th> <th>Frame 33 y</th> <th>Frame 34 x</th> <th>Frame 34 y</th> <th>Frame 35 x</th> <th>Frame 35 y</th> <th>Frame 36 x</th> <th>Frame 36 y</th> <th>Frame 37 x</th> <th>Frame 37 y</th> <th>Frame 38 x</th> <th>Frame 38 y</th> <th>Frame 39 x</th> <th>Frame 39 y</th> <th>Frame 40 x</th> <th>Frame 40 y</th> <th>Frame 41 x</th> <th>Frame 41 y</th> <th>Frame 42 x</th> <th>Frame 42 y</th> <th>Frame 43 x</th> <th>Frame 43 y</th> <th>Frame 44 x</th> <th>Frame 44 y</th> <th>Frame 45 x</th> <th>Frame 45 y</th> <th>Movement type</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0.522768 </td> <td>0.769731 </td> <td>0.536186 </td> <td>0.749446 </td> <td>0.518625 </td> <td>0.757197 </td> <td>0.517752 </td> <td>0.756847 </td> <td>0.504951 </td> <td>0.726008 </td> <td>0.50008  </td> <td>0.712113 </td> <td>0.463555 </td> <td>0.712355 </td> <td>0.49873  </td> <td>0.736872 </td> <td>0.51472  </td> <td>0.754353 </td> <td>0.517935  </td> <td>0.748163  </td> <td>0.5082    </td> <td>0.734278  </td> <td>0.50004   </td> <td>0.726941  </td> <td>0.49291   </td> <td>0.71189   </td> <td>0.480587  </td> <td>0.715755  </td> <td>0.476772  </td> <td>0.723531  </td> <td>0.504372  </td> <td>0.717318  </td> <td>0.46351   </td> <td>0.70031   </td> <td>0.463217  </td> <td>0.693279  </td> <td>0.474777  </td> <td>0.722122  </td> <td>0.512079  </td> <td>0.73267   </td> <td>0.506785  </td> <td>0.731242  </td> <td>0.497417  </td> <td>0.723703  </td> <td>0.505879  </td> <td>0.726615  </td> <td>0.51537   </td> <td>0.741874  </td> <td>0.544376  </td> <td>0.741177  </td> <td>0.51367   </td> <td>0.714379  </td> <td>0.509508  </td> <td>0.715222  </td> <td>0.519559  </td> <td>0.704945  </td> <td>0.511828  </td> <td>0.69361   </td> <td>0.511366  </td> <td>0.685024  </td> <td>0.510194  </td> <td>0.686122  </td> <td>0.518486  </td> <td>0.694125  </td> <td>0.524232  </td> <td>0.68817   </td> <td>0.531254  </td> <td>0.672905  </td> <td>0.530833  </td> <td>0.672029  </td> <td>0.521013  </td> <td>0.621037  </td> <td>0.481328  </td> <td>0.586983  </td> <td>0.450996  </td> <td>0.576725  </td> <td>0.474634  </td> <td>0.585757  </td> <td>0.465209  </td> <td>0.572517  </td> <td>0.430172  </td> <td>0.547155  </td> <td>0.429693  </td> <td>0.531896  </td> <td>0.415799  </td> <td>0.516734  </td> <td>0.40249   </td> <td>0.528653  </td> <td>0.413692  </td> <td>0.510434  </td> <td>vertical straight-line  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.179546 </td> <td>0.658986 </td> <td>0.177132 </td> <td>0.656834 </td> <td>0.168157 </td> <td>0.664803 </td> <td>0.176407 </td> <td>0.654713 </td> <td>0.167577 </td> <td>0.635559 </td> <td>0.138276 </td> <td>0.633621 </td> <td>0.143817 </td> <td>0.633303 </td> <td>0.154967 </td> <td>0.643993 </td> <td>0.169151 </td> <td>0.646888 </td> <td>0.138409  </td> <td>0.62286   </td> <td>0.141052  </td> <td>0.638818  </td> <td>0.129957  </td> <td>0.644284  </td> <td>0.141763  </td> <td>0.643459  </td> <td>0.127024  </td> <td>0.641122  </td> <td>0.133745  </td> <td>0.63458   </td> <td>0.114496  </td> <td>0.632741  </td> <td>0.0891234 </td> <td>0.631917  </td> <td>0.0836099 </td> <td>0.630901  </td> <td>0.07445   </td> <td>0.621396  </td> <td>0.072605  </td> <td>0.635247  </td> <td>0.0506362 </td> <td>0.620064  </td> <td>0.0467104 </td> <td>0.62067   </td> <td>0.0531715 </td> <td>0.645212  </td> <td>0.0374171 </td> <td>0.634352  </td> <td>0.0182681 </td> <td>0.61547   </td> <td>-0.0197023</td> <td>0.6088    </td> <td>-0.027299 </td> <td>0.605641  </td> <td>-0.0482872</td> <td>0.594468  </td> <td>-0.0640002</td> <td>0.588416  </td> <td>-0.0565593</td> <td>0.582703  </td> <td>-0.0881633</td> <td>0.586423  </td> <td>-0.0929613</td> <td>0.600561  </td> <td>-0.0928198</td> <td>0.609785  </td> <td>-0.107121 </td> <td>0.624372  </td> <td>-0.115449 </td> <td>0.613028  </td> <td>-0.140709 </td> <td>0.614448  </td> <td>-0.148999 </td> <td>0.607538  </td> <td>-0.179288 </td> <td>0.582983  </td> <td>-0.196426 </td> <td>0.612175  </td> <td>-0.195264 </td> <td>0.580151  </td> <td>-0.230368 </td> <td>0.577835  </td> <td>-0.250168 </td> <td>0.550737  </td> <td>-0.274717 </td> <td>0.571828  </td> <td>-0.258795 </td> <td>0.590663  </td> <td>-0.256045 </td> <td>0.578798  </td> <td>horizontal straight-line</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.805813 </td> <td>0.651365 </td> <td>0.832204 </td> <td>0.666023 </td> <td>0.834636 </td> <td>0.645757 </td> <td>0.826685 </td> <td>0.645685 </td> <td>0.816671 </td> <td>0.625701 </td> <td>0.810289 </td> <td>0.637001 </td> <td>0.819373 </td> <td>0.635922 </td> <td>0.827567 </td> <td>0.637587 </td> <td>0.813763 </td> <td>0.645346 </td> <td>0.824472  </td> <td>0.632012  </td> <td>0.82673   </td> <td>0.643524  </td> <td>0.817462  </td> <td>0.638418  </td> <td>0.804468  </td> <td>0.63604   </td> <td>0.830122  </td> <td>0.652033  </td> <td>0.828967  </td> <td>0.658297  </td> <td>0.850648  </td> <td>0.678696  </td> <td>0.845375  </td> <td>0.679893  </td> <td>0.858148  </td> <td>0.677961  </td> <td>0.852067  </td> <td>0.673301  </td> <td>0.849921  </td> <td>0.668893  </td> <td>0.84142   </td> <td>0.681652  </td> <td>0.869216  </td> <td>0.68519   </td> <td>0.857929  </td> <td>0.69222   </td> <td>0.868462  </td> <td>0.683252  </td> <td>0.843773  </td> <td>0.668541  </td> <td>0.848835  </td> <td>0.674522  </td> <td>0.843266  </td> <td>0.663946  </td> <td>0.830001  </td> <td>0.655817  </td> <td>0.825753  </td> <td>0.654858  </td> <td>0.822624  </td> <td>0.660058  </td> <td>0.818284  </td> <td>0.643763  </td> <td>0.796939  </td> <td>0.62913   </td> <td>0.789691  </td> <td>0.61749   </td> <td>0.772315  </td> <td>0.606656  </td> <td>0.773609  </td> <td>0.605172  </td> <td>0.76006   </td> <td>0.579637  </td> <td>0.728993  </td> <td>0.576794  </td> <td>0.726034  </td> <td>0.584777  </td> <td>0.705394  </td> <td>0.573393  </td> <td>0.693345  </td> <td>0.579456  </td> <td>0.693249  </td> <td>0.581378  </td> <td>0.684606  </td> <td>0.576406  </td> <td>0.670061  </td> <td>0.566151  </td> <td>0.642557  </td> <td>0.569876  </td> <td>0.629915  </td> <td>0.561387  </td> <td>horizontal straight-line</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.83942  </td> <td>0.564511 </td> <td>0.853031 </td> <td>0.560031 </td> <td>0.845024 </td> <td>0.549989 </td> <td>0.824814 </td> <td>0.546812 </td> <td>0.821869 </td> <td>0.5462   </td> <td>0.820898 </td> <td>0.536278 </td> <td>0.800887 </td> <td>0.525634 </td> <td>0.801667 </td> <td>0.542531 </td> <td>0.806793 </td> <td>0.553656 </td> <td>0.799924  </td> <td>0.576862  </td> <td>0.810348  </td> <td>0.571102  </td> <td>0.801704  </td> <td>0.57294   </td> <td>0.773529  </td> <td>0.561476  </td> <td>0.772628  </td> <td>0.565349  </td> <td>0.773298  </td> <td>0.566374  </td> <td>0.727042  </td> <td>0.553929  </td> <td>0.723279  </td> <td>0.579006  </td> <td>0.731698  </td> <td>0.593158  </td> <td>0.727945  </td> <td>0.606501  </td> <td>0.72577   </td> <td>0.644594  </td> <td>0.721218  </td> <td>0.642742  </td> <td>0.718306  </td> <td>0.65346   </td> <td>0.702917  </td> <td>0.676261  </td> <td>0.724201  </td> <td>0.707004  </td> <td>0.711995  </td> <td>0.708004  </td> <td>0.703505  </td> <td>0.708526  </td> <td>0.697355  </td> <td>0.711636  </td> <td>0.674235  </td> <td>0.737123  </td> <td>0.68839   </td> <td>0.735325  </td> <td>0.682767  </td> <td>0.741957  </td> <td>0.671688  </td> <td>0.739555  </td> <td>0.634614  </td> <td>0.737214  </td> <td>0.605281  </td> <td>0.713473  </td> <td>0.592041  </td> <td>0.713161  </td> <td>0.561725  </td> <td>0.714786  </td> <td>0.538708  </td> <td>0.703583  </td> <td>0.531588  </td> <td>0.718057  </td> <td>0.553363  </td> <td>0.737859  </td> <td>0.539013  </td> <td>0.719495  </td> <td>0.513489  </td> <td>0.721538  </td> <td>0.503373  </td> <td>0.719414  </td> <td>0.504463  </td> <td>0.731782  </td> <td>0.514171  </td> <td>0.730937  </td> <td>0.518139  </td> <td>0.738488  </td> <td>0.503466  </td> <td>0.730267  </td> <td>horizontal straight-line</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.5504   </td> <td>0.724639 </td> <td>0.548864 </td> <td>0.727437 </td> <td>0.559092 </td> <td>0.757221 </td> <td>0.576803 </td> <td>0.763471 </td> <td>0.579116 </td> <td>0.752175 </td> <td>0.581021 </td> <td>0.771376 </td> <td>0.588351 </td> <td>0.773922 </td> <td>0.604139 </td> <td>0.782165 </td> <td>0.603875 </td> <td>0.768626 </td> <td>0.608751  </td> <td>0.74764   </td> <td>0.601986  </td> <td>0.732743  </td> <td>0.599202  </td> <td>0.717549  </td> <td>0.607302  </td> <td>0.721427  </td> <td>0.620328  </td> <td>0.682498  </td> <td>0.603376  </td> <td>0.66756   </td> <td>0.61182   </td> <td>0.641005  </td> <td>0.571499  </td> <td>0.605139  </td> <td>0.563333  </td> <td>0.55631   </td> <td>0.532991  </td> <td>0.52395   </td> <td>0.514682  </td> <td>0.500591  </td> <td>0.530536  </td> <td>0.486458  </td> <td>0.522758  </td> <td>0.453329  </td> <td>0.515001  </td> <td>0.412563  </td> <td>0.502188  </td> <td>0.39027   </td> <td>0.503148  </td> <td>0.368665  </td> <td>0.501019  </td> <td>0.346839  </td> <td>0.512556  </td> <td>0.312493  </td> <td>0.47574   </td> <td>0.279755  </td> <td>0.476174  </td> <td>0.257592  </td> <td>0.473331  </td> <td>0.23701   </td> <td>0.492565  </td> <td>0.245318  </td> <td>0.510208  </td> <td>0.231261  </td> <td>0.509312  </td> <td>0.21478   </td> <td>0.507778  </td> <td>0.202246  </td> <td>0.506741  </td> <td>0.192624  </td> <td>0.502328  </td> <td>0.170399  </td> <td>0.488535  </td> <td>0.143743  </td> <td>0.495343  </td> <td>0.156119  </td> <td>0.510498  </td> <td>0.17154   </td> <td>0.538879  </td> <td>0.160089  </td> <td>0.531483  </td> <td>0.171206  </td> <td>0.55924   </td> <td>0.159821  </td> <td>0.539761  </td> <td>0.153518  </td> <td>0.520628  </td> <td>0.133368  </td> <td>0.503185  </td> <td>0.112633  </td> <td>vertical straight-line  </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Frame 1 x | Frame 1 y | Frame 2 x | Frame 2 y | Frame 3 x | Frame 3 y | Frame 4 x | Frame 4 y | Frame 5 x | Frame 5 y | Frame 6 x | Frame 6 y | Frame 7 x | Frame 7 y | Frame 8 x | Frame 8 y | Frame 9 x | Frame 9 y | Frame 10 x | Frame 10 y | Frame 11 x | Frame 11 y | Frame 12 x | Frame 12 y | Frame 13 x | Frame 13 y | Frame 14 x | Frame 14 y | Frame 15 x | Frame 15 y | Frame 16 x | Frame 16 y | Frame 17 x | Frame 17 y | Frame 18 x | Frame 18 y | Frame 19 x | Frame 19 y | Frame 20 x | Frame 20 y | Frame 21 x | Frame 21 y | Frame 22 x | Frame 22 y | Frame 23 x | Frame 23 y | Frame 24 x | Frame 24 y | Frame 25 x | Frame 25 y | Frame 26 x | Frame 26 y | Frame 27 x | Frame 27 y | Frame 28 x | Frame 28 y | Frame 29 x | Frame 29 y | Frame 30 x | Frame 30 y | Frame 31 x | Frame 31 y | Frame 32 x | Frame 32 y | Frame 33 x | Frame 33 y | Frame 34 x | Frame 34 y | Frame 35 x | Frame 35 y | Frame 36 x | Frame 36 y | Frame 37 x | Frame 37 y | Frame 38 x | Frame 38 y | Frame 39 x | Frame 39 y | Frame 40 x | Frame 40 y | Frame 41 x | Frame 41 y | Frame 42 x | Frame 42 y | Frame 43 x | Frame 43 y | Frame 44 x | Frame 44 y | Frame 45 x | Frame 45 y | Movement type\n",
       "0.522768  | 0.769731  | 0.536186  | 0.749446  | 0.518625  | 0.757197  | 0.517752  | 0.756847  | 0.504951  | 0.726008  | 0.50008   | 0.712113  | 0.463555  | 0.712355  | 0.49873   | 0.736872  | 0.51472   | 0.754353  | 0.517935   | 0.748163   | 0.5082     | 0.734278   | 0.50004    | 0.726941   | 0.49291    | 0.71189    | 0.480587   | 0.715755   | 0.476772   | 0.723531   | 0.504372   | 0.717318   | 0.46351    | 0.70031    | 0.463217   | 0.693279   | 0.474777   | 0.722122   | 0.512079   | 0.73267    | 0.506785   | 0.731242   | 0.497417   | 0.723703   | 0.505879   | 0.726615   | 0.51537    | 0.741874   | 0.544376   | 0.741177   | 0.51367    | 0.714379   | 0.509508   | 0.715222   | 0.519559   | 0.704945   | 0.511828   | 0.69361    | 0.511366   | 0.685024   | 0.510194   | 0.686122   | 0.518486   | 0.694125   | 0.524232   | 0.68817    | 0.531254   | 0.672905   | 0.530833   | 0.672029   | 0.521013   | 0.621037   | 0.481328   | 0.586983   | 0.450996   | 0.576725   | 0.474634   | 0.585757   | 0.465209   | 0.572517   | 0.430172   | 0.547155   | 0.429693   | 0.531896   | 0.415799   | 0.516734   | 0.40249    | 0.528653   | 0.413692   | 0.510434   | vertical straight-line\n",
       "0.179546  | 0.658986  | 0.177132  | 0.656834  | 0.168157  | 0.664803  | 0.176407  | 0.654713  | 0.167577  | 0.635559  | 0.138276  | 0.633621  | 0.143817  | 0.633303  | 0.154967  | 0.643993  | 0.169151  | 0.646888  | 0.138409   | 0.62286    | 0.141052   | 0.638818   | 0.129957   | 0.644284   | 0.141763   | 0.643459   | 0.127024   | 0.641122   | 0.133745   | 0.63458    | 0.114496   | 0.632741   | 0.0891234  | 0.631917   | 0.0836099  | 0.630901   | 0.07445    | 0.621396   | 0.072605   | 0.635247   | 0.0506362  | 0.620064   | 0.0467104  | 0.62067    | 0.0531715  | 0.645212   | 0.0374171  | 0.634352   | 0.0182681  | 0.61547    | -0.0197023 | 0.6088     | -0.027299  | 0.605641   | -0.0482872 | 0.594468   | -0.0640002 | 0.588416   | -0.0565593 | 0.582703   | -0.0881633 | 0.586423   | -0.0929613 | 0.600561   | -0.0928198 | 0.609785   | -0.107121  | 0.624372   | -0.115449  | 0.613028   | -0.140709  | 0.614448   | -0.148999  | 0.607538   | -0.179288  | 0.582983   | -0.196426  | 0.612175   | -0.195264  | 0.580151   | -0.230368  | 0.577835   | -0.250168  | 0.550737   | -0.274717  | 0.571828   | -0.258795  | 0.590663   | -0.256045  | 0.578798   | horizontal straight-line\n",
       "0.805813  | 0.651365  | 0.832204  | 0.666023  | 0.834636  | 0.645757  | 0.826685  | 0.645685  | 0.816671  | 0.625701  | 0.810289  | 0.637001  | 0.819373  | 0.635922  | 0.827567  | 0.637587  | 0.813763  | 0.645346  | 0.824472   | 0.632012   | 0.82673    | 0.643524   | 0.817462   | 0.638418   | 0.804468   | 0.63604    | 0.830122   | 0.652033   | 0.828967   | 0.658297   | 0.850648   | 0.678696   | 0.845375   | 0.679893   | 0.858148   | 0.677961   | 0.852067   | 0.673301   | 0.849921   | 0.668893   | 0.84142    | 0.681652   | 0.869216   | 0.68519    | 0.857929   | 0.69222    | 0.868462   | 0.683252   | 0.843773   | 0.668541   | 0.848835   | 0.674522   | 0.843266   | 0.663946   | 0.830001   | 0.655817   | 0.825753   | 0.654858   | 0.822624   | 0.660058   | 0.818284   | 0.643763   | 0.796939   | 0.62913    | 0.789691   | 0.61749    | 0.772315   | 0.606656   | 0.773609   | 0.605172   | 0.76006    | 0.579637   | 0.728993   | 0.576794   | 0.726034   | 0.584777   | 0.705394   | 0.573393   | 0.693345   | 0.579456   | 0.693249   | 0.581378   | 0.684606   | 0.576406   | 0.670061   | 0.566151   | 0.642557   | 0.569876   | 0.629915   | 0.561387   | horizontal straight-line\n",
       "0.83942   | 0.564511  | 0.853031  | 0.560031  | 0.845024  | 0.549989  | 0.824814  | 0.546812  | 0.821869  | 0.5462    | 0.820898  | 0.536278  | 0.800887  | 0.525634  | 0.801667  | 0.542531  | 0.806793  | 0.553656  | 0.799924   | 0.576862   | 0.810348   | 0.571102   | 0.801704   | 0.57294    | 0.773529   | 0.561476   | 0.772628   | 0.565349   | 0.773298   | 0.566374   | 0.727042   | 0.553929   | 0.723279   | 0.579006   | 0.731698   | 0.593158   | 0.727945   | 0.606501   | 0.72577    | 0.644594   | 0.721218   | 0.642742   | 0.718306   | 0.65346    | 0.702917   | 0.676261   | 0.724201   | 0.707004   | 0.711995   | 0.708004   | 0.703505   | 0.708526   | 0.697355   | 0.711636   | 0.674235   | 0.737123   | 0.68839    | 0.735325   | 0.682767   | 0.741957   | 0.671688   | 0.739555   | 0.634614   | 0.737214   | 0.605281   | 0.713473   | 0.592041   | 0.713161   | 0.561725   | 0.714786   | 0.538708   | 0.703583   | 0.531588   | 0.718057   | 0.553363   | 0.737859   | 0.539013   | 0.719495   | 0.513489   | 0.721538   | 0.503373   | 0.719414   | 0.504463   | 0.731782   | 0.514171   | 0.730937   | 0.518139   | 0.738488   | 0.503466   | 0.730267   | horizontal straight-line\n",
       "0.5504    | 0.724639  | 0.548864  | 0.727437  | 0.559092  | 0.757221  | 0.576803  | 0.763471  | 0.579116  | 0.752175  | 0.581021  | 0.771376  | 0.588351  | 0.773922  | 0.604139  | 0.782165  | 0.603875  | 0.768626  | 0.608751   | 0.74764    | 0.601986   | 0.732743   | 0.599202   | 0.717549   | 0.607302   | 0.721427   | 0.620328   | 0.682498   | 0.603376   | 0.66756    | 0.61182    | 0.641005   | 0.571499   | 0.605139   | 0.563333   | 0.55631    | 0.532991   | 0.52395    | 0.514682   | 0.500591   | 0.530536   | 0.486458   | 0.522758   | 0.453329   | 0.515001   | 0.412563   | 0.502188   | 0.39027    | 0.503148   | 0.368665   | 0.501019   | 0.346839   | 0.512556   | 0.312493   | 0.47574    | 0.279755   | 0.476174   | 0.257592   | 0.473331   | 0.23701    | 0.492565   | 0.245318   | 0.510208   | 0.231261   | 0.509312   | 0.21478    | 0.507778   | 0.202246   | 0.506741   | 0.192624   | 0.502328   | 0.170399   | 0.488535   | 0.143743   | 0.495343   | 0.156119   | 0.510498   | 0.17154    | 0.538879   | 0.160089   | 0.531483   | 0.171206   | 0.55924    | 0.159821   | 0.539761   | 0.153518   | 0.520628   | 0.133368   | 0.503185   | 0.112633   | vertical straight-line"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movements = Table.read_table(\"movements.csv\")\n",
    "movements.take(np.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create train and test sets\n",
    "\n",
    "First, let's split up the data into train and test sets. For this assignment, we will do a simple holdout set, assigning a random 20% of the data as the test data, and building the model on the remaining 80% of the data. \n",
    "\n",
    "<font color = 'red'>**Question 1. Create two Tables, one called `test` and one called `train`. The `test` table should contain a random 20% of the data, while the `train` Table should contain the other 80%.** <\\font>\n",
    "\n",
    "*Hint:* You can shuffle the entire dataset (sample the whole dataset without replacement), then just take the top 20% as your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movements.num_rows * .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "       205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "       218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230,\n",
       "       231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243,\n",
       "       244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,\n",
       "       257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
       "       270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282,\n",
       "       283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
       "       296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "       309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "       322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334,\n",
       "       335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347,\n",
       "       348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360,\n",
       "       361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373,\n",
       "       374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386,\n",
       "       387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399,\n",
       "       400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
       "       413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
       "       426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
       "       439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "       452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464,\n",
       "       465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477,\n",
       "       478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
       "       491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
       "       504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516,\n",
       "       517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529,\n",
       "       530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542,\n",
       "       543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555,\n",
       "       556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568,\n",
       "       569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581,\n",
       "       582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "       595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607,\n",
       "       608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620,\n",
       "       621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633,\n",
       "       634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646,\n",
       "       647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659,\n",
       "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
       "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
       "       686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698,\n",
       "       699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711,\n",
       "       712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724,\n",
       "       725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737,\n",
       "       738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750,\n",
       "       751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n",
       "       764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776,\n",
       "       777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789,\n",
       "       790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802,\n",
       "       803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815,\n",
       "       816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828,\n",
       "       829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841,\n",
       "       842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854,\n",
       "       855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867,\n",
       "       868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880,\n",
       "       881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893,\n",
       "       894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906,\n",
       "       907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919,\n",
       "       920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932,\n",
       "       933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945,\n",
       "       946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958,\n",
       "       959])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(192, movements.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_movements = movements.sample(with_replacement = False)\n",
    "test = shuffled_movements.take(np.arange(192))\n",
    "train = shuffled_movements.take(np.arange(192, movements.num_rows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fitting the models\n",
    "\n",
    "Let's fit three different models: Logistic Regression, Decision Trees, and K-Nearest Neighbors. The model objects are initialized using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model objects\n",
    "logit = LogisticRegression(solver = 'liblinear')\n",
    "tree = DecisionTreeClassifier()\n",
    "knn = KNeighborsClassifier(n_neighbors = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the default values for many of these. We generally would want to try a variety of different models with different parameters (for example, using different values for the stopping criteria in decision trees). We'll stick with just the defaults for now for this assignments. You can now use the `.fit` method to give it the data. \n",
    "\n",
    "<font color = 'red'>**Question 2. Using the `train` Table you created above, fit each of the three models.**</font>\n",
    "\n",
    "If you're not sure about the exact format of the data needed, remember that you need to use `.rows` for the `X` values and `.column` for the `y` values. See lecture material and Labs 8 and 9 for how we fit linear regression models if you're still unsure of how to proceed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Frame 1 x</th> <th>Frame 1 y</th> <th>Frame 2 x</th> <th>Frame 2 y</th> <th>Frame 3 x</th> <th>Frame 3 y</th> <th>Frame 4 x</th> <th>Frame 4 y</th> <th>Frame 5 x</th> <th>Frame 5 y</th> <th>Frame 6 x</th> <th>Frame 6 y</th> <th>Frame 7 x</th> <th>Frame 7 y</th> <th>Frame 8 x</th> <th>Frame 8 y</th> <th>Frame 9 x</th> <th>Frame 9 y</th> <th>Frame 10 x</th> <th>Frame 10 y</th> <th>Frame 11 x</th> <th>Frame 11 y</th> <th>Frame 12 x</th> <th>Frame 12 y</th> <th>Frame 13 x</th> <th>Frame 13 y</th> <th>Frame 14 x</th> <th>Frame 14 y</th> <th>Frame 15 x</th> <th>Frame 15 y</th> <th>Frame 16 x</th> <th>Frame 16 y</th> <th>Frame 17 x</th> <th>Frame 17 y</th> <th>Frame 18 x</th> <th>Frame 18 y</th> <th>Frame 19 x</th> <th>Frame 19 y</th> <th>Frame 20 x</th> <th>Frame 20 y</th> <th>Frame 21 x</th> <th>Frame 21 y</th> <th>Frame 22 x</th> <th>Frame 22 y</th> <th>Frame 23 x</th> <th>Frame 23 y</th> <th>Frame 24 x</th> <th>Frame 24 y</th> <th>Frame 25 x</th> <th>Frame 25 y</th> <th>Frame 26 x</th> <th>Frame 26 y</th> <th>Frame 27 x</th> <th>Frame 27 y</th> <th>Frame 28 x</th> <th>Frame 28 y</th> <th>Frame 29 x</th> <th>Frame 29 y</th> <th>Frame 30 x</th> <th>Frame 30 y</th> <th>Frame 31 x</th> <th>Frame 31 y</th> <th>Frame 32 x</th> <th>Frame 32 y</th> <th>Frame 33 x</th> <th>Frame 33 y</th> <th>Frame 34 x</th> <th>Frame 34 y</th> <th>Frame 35 x</th> <th>Frame 35 y</th> <th>Frame 36 x</th> <th>Frame 36 y</th> <th>Frame 37 x</th> <th>Frame 37 y</th> <th>Frame 38 x</th> <th>Frame 38 y</th> <th>Frame 39 x</th> <th>Frame 39 y</th> <th>Frame 40 x</th> <th>Frame 40 y</th> <th>Frame 41 x</th> <th>Frame 41 y</th> <th>Frame 42 x</th> <th>Frame 42 y</th> <th>Frame 43 x</th> <th>Frame 43 y</th> <th>Frame 44 x</th> <th>Frame 44 y</th> <th>Frame 45 x</th> <th>Frame 45 y</th> <th>Movement type</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0.71485  </td> <td>0.765043 </td> <td>0.737099 </td> <td>0.780259 </td> <td>0.742681 </td> <td>0.780098 </td> <td>0.73946  </td> <td>0.779346 </td> <td>0.71933  </td> <td>0.771506 </td> <td>0.736324 </td> <td>0.775486 </td> <td>0.730072 </td> <td>0.771722 </td> <td>0.744539 </td> <td>0.796989 </td> <td>0.727805 </td> <td>0.771922 </td> <td>0.71896   </td> <td>0.790602  </td> <td>0.71778   </td> <td>0.779097  </td> <td>0.688825  </td> <td>0.751317  </td> <td>0.679275  </td> <td>0.763585  </td> <td>0.642987  </td> <td>0.74896   </td> <td>0.641377  </td> <td>0.774647  </td> <td>0.628758  </td> <td>0.781192  </td> <td>0.621349  </td> <td>0.768986  </td> <td>0.595278  </td> <td>0.774901  </td> <td>0.595864  </td> <td>0.805988  </td> <td>0.590459  </td> <td>0.811827  </td> <td>0.577786  </td> <td>0.809678  </td> <td>0.532416  </td> <td>0.79249   </td> <td>0.500941  </td> <td>0.784361  </td> <td>0.496627  </td> <td>0.784489  </td> <td>0.475813  </td> <td>0.808039  </td> <td>0.473828  </td> <td>0.804328  </td> <td>0.47181   </td> <td>0.815833  </td> <td>0.445607  </td> <td>0.802985  </td> <td>0.410586  </td> <td>0.811969  </td> <td>0.406083  </td> <td>0.807403  </td> <td>0.379157  </td> <td>0.818928  </td> <td>0.381839  </td> <td>0.806682  </td> <td>0.36873   </td> <td>0.820335  </td> <td>0.35693   </td> <td>0.828528  </td> <td>0.352931  </td> <td>0.835654  </td> <td>0.330731  </td> <td>0.832286  </td> <td>0.308226  </td> <td>0.813073  </td> <td>0.293601  </td> <td>0.822417  </td> <td>0.286986  </td> <td>0.82254   </td> <td>0.282261  </td> <td>0.817571  </td> <td>0.268589  </td> <td>0.800004  </td> <td>0.227901  </td> <td>0.787924  </td> <td>0.209931  </td> <td>0.814342  </td> <td>0.203052  </td> <td>0.803519  </td> <td>0.203618  </td> <td>0.798781  </td> <td>horizontal straight-line</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.69456  </td> <td>0.503723 </td> <td>0.703451 </td> <td>0.503219 </td> <td>0.709287 </td> <td>0.517765 </td> <td>0.712228 </td> <td>0.508654 </td> <td>0.709911 </td> <td>0.493239 </td> <td>0.692369 </td> <td>0.501736 </td> <td>0.711034 </td> <td>0.49124  </td> <td>0.693827 </td> <td>0.488496 </td> <td>0.696958 </td> <td>0.487491 </td> <td>0.680477  </td> <td>0.489306  </td> <td>0.65449   </td> <td>0.494359  </td> <td>0.66519   </td> <td>0.50919   </td> <td>0.657931  </td> <td>0.509118  </td> <td>0.632741  </td> <td>0.504688  </td> <td>0.604124  </td> <td>0.496703  </td> <td>0.588956  </td> <td>0.490808  </td> <td>0.573937  </td> <td>0.505938  </td> <td>0.572397  </td> <td>0.501944  </td> <td>0.550131  </td> <td>0.518629  </td> <td>0.544682  </td> <td>0.524164  </td> <td>0.534684  </td> <td>0.508335  </td> <td>0.517595  </td> <td>0.531931  </td> <td>0.503313  </td> <td>0.53364   </td> <td>0.485627  </td> <td>0.513646  </td> <td>0.438797  </td> <td>0.496117  </td> <td>0.41378   </td> <td>0.508976  </td> <td>0.411885  </td> <td>0.5121    </td> <td>0.395748  </td> <td>0.520311  </td> <td>0.374709  </td> <td>0.5186    </td> <td>0.330692  </td> <td>0.48685   </td> <td>0.279736  </td> <td>0.461089  </td> <td>0.243994  </td> <td>0.477048  </td> <td>0.235387  </td> <td>0.465851  </td> <td>0.211617  </td> <td>0.45898   </td> <td>0.195191  </td> <td>0.47896   </td> <td>0.191314  </td> <td>0.486825  </td> <td>0.161328  </td> <td>0.472207  </td> <td>0.120716  </td> <td>0.460705  </td> <td>0.107514  </td> <td>0.48495   </td> <td>0.0951534 </td> <td>0.49577   </td> <td>0.0745596 </td> <td>0.499548  </td> <td>0.0495125 </td> <td>0.49421   </td> <td>0.0200997 </td> <td>0.500994  </td> <td>0.0231432 </td> <td>0.495738  </td> <td>0.0200144 </td> <td>0.502012  </td> <td>horizontal straight-line</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.616524 </td> <td>0.749343 </td> <td>0.627304 </td> <td>0.750988 </td> <td>0.61076  </td> <td>0.751783 </td> <td>0.638413 </td> <td>0.759778 </td> <td>0.649615 </td> <td>0.801231 </td> <td>0.695376 </td> <td>0.81938  </td> <td>0.687806 </td> <td>0.802791 </td> <td>0.664587 </td> <td>0.803993 </td> <td>0.674821 </td> <td>0.803104 </td> <td>0.687685  </td> <td>0.818772  </td> <td>0.714221  </td> <td>0.824487  </td> <td>0.717358  </td> <td>0.812281  </td> <td>0.712002  </td> <td>0.803336  </td> <td>0.704383  </td> <td>0.799217  </td> <td>0.701208  </td> <td>0.788683  </td> <td>0.687776  </td> <td>0.773949  </td> <td>0.690682  </td> <td>0.782144  </td> <td>0.718288  </td> <td>0.771288  </td> <td>0.708967  </td> <td>0.766056  </td> <td>0.710361  </td> <td>0.744262  </td> <td>0.705778  </td> <td>0.724299  </td> <td>0.675466  </td> <td>0.703308  </td> <td>0.689834  </td> <td>0.690912  </td> <td>0.66955   </td> <td>0.650584  </td> <td>0.698738  </td> <td>0.655619  </td> <td>0.693462  </td> <td>0.618911  </td> <td>0.670931  </td> <td>0.572607  </td> <td>0.677534  </td> <td>0.564706  </td> <td>0.694533  </td> <td>0.560779  </td> <td>0.694842  </td> <td>0.545617  </td> <td>0.703936  </td> <td>0.535652  </td> <td>0.703216  </td> <td>0.509506  </td> <td>0.723055  </td> <td>0.505336  </td> <td>0.724919  </td> <td>0.478191  </td> <td>0.718228  </td> <td>0.461444  </td> <td>0.722413  </td> <td>0.4283    </td> <td>0.724837  </td> <td>0.424491  </td> <td>0.717566  </td> <td>0.387996  </td> <td>0.719239  </td> <td>0.38553   </td> <td>0.729586  </td> <td>0.379779  </td> <td>0.738334  </td> <td>0.343997  </td> <td>0.722802  </td> <td>0.348802  </td> <td>0.733966  </td> <td>0.318945  </td> <td>0.740289  </td> <td>0.302709  </td> <td>0.741837  </td> <td>0.290825  </td> <td>vertical straight-line  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.574706 </td> <td>0.7079   </td> <td>0.572501 </td> <td>0.713168 </td> <td>0.586835 </td> <td>0.70903  </td> <td>0.557148 </td> <td>0.696211 </td> <td>0.548032 </td> <td>0.671017 </td> <td>0.524572 </td> <td>0.65228  </td> <td>0.525832 </td> <td>0.630099 </td> <td>0.501776 </td> <td>0.612953 </td> <td>0.495346 </td> <td>0.617049 </td> <td>0.509899  </td> <td>0.623357  </td> <td>0.5002    </td> <td>0.587577  </td> <td>0.480941  </td> <td>0.588938  </td> <td>0.494571  </td> <td>0.571862  </td> <td>0.486017  </td> <td>0.554632  </td> <td>0.489724  </td> <td>0.546657  </td> <td>0.505741  </td> <td>0.538665  </td> <td>0.494026  </td> <td>0.521794  </td> <td>0.485543  </td> <td>0.499917  </td> <td>0.489176  </td> <td>0.482717  </td> <td>0.486385  </td> <td>0.464748  </td> <td>0.491686  </td> <td>0.441061  </td> <td>0.489745  </td> <td>0.433722  </td> <td>0.482038  </td> <td>0.411105  </td> <td>0.476429  </td> <td>0.397741  </td> <td>0.480865  </td> <td>0.37566   </td> <td>0.471457  </td> <td>0.359551  </td> <td>0.480057  </td> <td>0.345611  </td> <td>0.472088  </td> <td>0.333241  </td> <td>0.458846  </td> <td>0.317516  </td> <td>0.451819  </td> <td>0.278136  </td> <td>0.427985  </td> <td>0.255853  </td> <td>0.43785   </td> <td>0.252271  </td> <td>0.427562  </td> <td>0.235172  </td> <td>0.408256  </td> <td>0.225208  </td> <td>0.414609  </td> <td>0.212277  </td> <td>0.40872   </td> <td>0.193409  </td> <td>0.405234  </td> <td>0.175933  </td> <td>0.407859  </td> <td>0.180448  </td> <td>0.410483  </td> <td>0.164901  </td> <td>0.408688  </td> <td>0.142275  </td> <td>0.408616  </td> <td>0.158161  </td> <td>0.44192   </td> <td>0.152186  </td> <td>0.42194   </td> <td>0.132049  </td> <td>0.410869  </td> <td>0.103154  </td> <td>0.420523  </td> <td>0.117705  </td> <td>vertical straight-line  </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (764 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.drop('Movement type').rows # predictor\n",
    "label = train.column('Movement type') == 'horizontal straight-line' # outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X = features, y = label)\n",
    "tree.fit(X = features, y = label)\n",
    "knn.fit(X = features, y = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you fit the models, you can use the `.classes_` instance variable to check the classes that are being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Predict Test Set\n",
    "\n",
    "Just as with linear regression, we can use the `.predict_proba` method with the model objects. The `.predict_proba` method provides scores for each class. We need to select one of the two classes as the \"positive\" outcome. Since there isn't one outcome we're particularly interested in out of the two possibilities, we will just choose \"horizontal straight-line\" to be positive. \n",
    "\n",
    "> In other situations, there might be one that you care more about. For example, if you are trying to predict recidivism in order to provide social programs to help released prisoners at higher risk of returning, then your \"positive\" outcome -- the outcome you are trying to predict -- would be \"return\", so that is what you would want to set as `True` below. \n",
    "\n",
    "We can't simply evaluate using the scores, though. We need to set a threshold so that we predict that an observation is \"horizontal straight-line\" or not. Here, we will set the threshold to be 0.5, but remember, there is nothing special about this number. We could have easily selected any other number between 0 and 1, and we should, in practice, try a wide range of values and see how our models perform with each of these thresholds.\n",
    "\n",
    "An example using K-Nearest Neighbors is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a threshold (above means predicted to be horizontal straight-line)\n",
    "threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you fit the model before running this!\n",
    "test_features = test.select(np.arange(0,90)).rows\n",
    "# test_features = test.drop('Movement type').rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.71428571, 0.28571429],\n",
       "       [0.57142857, 0.42857143],\n",
       "       [0.71428571, 0.28571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [0.28571429, 0.71428571],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.57142857, 0.42857143],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.14285714, 0.85714286],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.71428571, 0.28571429],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.71428571, 0.28571429],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.28571429, 0.71428571],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [0.14285714, 0.85714286],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.57142857, 0.42857143],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14285714, 0.85714286],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.71428571, 0.28571429],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.71428571, 0.28571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predicted = knn.predict_proba(test_features)[:,1] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True, False, False, False,  True,  True,\n",
       "       False, False,  True,  True,  True,  True, False, False,  True,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True, False,\n",
       "       False, False, False,  True, False,  True, False,  True,  True,\n",
       "       False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "        True,  True,  True, False, False,  True, False,  True,  True,\n",
       "        True, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False, False, False, False,  True,  True,  True,  True,\n",
       "       False, False, False,  True,  True,  True, False,  True,  True,\n",
       "       False, False, False,  True, False,  True,  True, False,  True,\n",
       "        True,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True, False, False,  True,  True, False,  True,  True, False,\n",
       "       False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "       False, False, False, False,  True, False,  True,  True, False,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "       False, False,  True])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = test.column('Movement type') == 'horizontal straight-line'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are only taking the first value from each row, as evidenced by the `[:,0]`. This is because we want the predicted scores for horizontal straight-line, and that is the first value (see the `.classes_` instance variable above). The scores for each of the categories are given, but we only really care about one, because that's all we need since the scores will add up to 1 across each row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 3. Create an array called `expected` that contain the expected values based on the test set. The `expected` array should contain `True` if the observation is actually a \"horizontal straight-line\" and `False` otherwise. Note that the `expected` array is based entirely on the real dataset, and not on an predictions! Create additional arrays that contain the predicted values for each of the models that we've fit (call them `logit_predicted` and `tree_predicted`).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True,  True, False, False,  True,  True,\n",
       "       False, False,  True,  True,  True,  True, False, False,  True,\n",
       "       False, False, False, False, False,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True,  True, False,\n",
       "       False, False,  True,  True, False,  True, False,  True,  True,\n",
       "       False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "       False,  True, False,  True, False,  True, False,  True, False,\n",
       "        True,  True,  True, False,  True,  True, False,  True,  True,\n",
       "        True, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False, False,  True,  True,  True,  True,\n",
       "       False, False, False,  True,  True,  True, False,  True,  True,\n",
       "       False, False, False,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True, False,  True,  True, False,\n",
       "       False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True, False,  True, False, False,\n",
       "       False, False, False,  True, False,  True, False,  True,  True,\n",
       "       False, False, False, False,  True,  True,  True,  True, False,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "        True, False, False, False, False,  True,  True,  True,  True,\n",
       "        True, False,  True])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.2\n",
    "logit_predicted = logit.predict_proba(test_features)[:,1] > threshold\n",
    "logit_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 82,   7],\n",
       "       [  0, 103]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(expected, logit_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363636363636364"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(expected, logit_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a confusion matrix using the `confusion_matrix` function that we brought at the beginning. This is part of the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(expected,knn_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[89,  0],\n",
       "       [ 8, 95]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(expected,knn_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns represent predictions and the rows represent actual values, so the top left is true negatives, the bottom right is true positives, the top right is false positives, and the bottom left is false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics\n",
    "\n",
    "Two metrics that are often more relevant than overall accuracy are **precision** and **recall**. \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds we can vary and tune the precision and recall of a given classifier. A conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall. \n",
    "\n",
    "We can use the `precision_score` and `recall_score` functions to find the value of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(expected,knn_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9223300970873787"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(expected,knn_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 4. Find the confusion matrix for the Logistic Regression and Decision Tree models. Using the confusion matrix, compute the accuracy, precision, and recall. Afterwards, use the `precision_score` and `recall_score` functions to verify your answer.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Repeating the steps\n",
    "\n",
    "We've done one iteration ... but we've only done it with one threshold, and we haven't tuned the parameters much. We won't go through all of the various ways we can fine-tune our models, but we can show how it is done: using loops.\n",
    "\n",
    "<font color = 'red'>**Question 5. Write a loop that tries thresholds of .1, .3, .5, .7, and .9. Store the precision of each model at each threshold within their own arrays, named `knn_precision`, `logit_precision` and `tree_precision`. Do the same for recall (replacing precision with recall in the naming convention).**</font>\n",
    "\n",
    "The loop has been started below for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_precision = make_array()\n",
    "logit_precision = make_array()\n",
    "tree_precision = make_array()\n",
    "    \n",
    "knn_recall = make_array()\n",
    "logit_recall = make_array()\n",
    "tree_recall = make_array()\n",
    "\n",
    "# Set up models and fit them here\n",
    "# Create the model objects\n",
    "logit = LogisticRegression(solver = 'liblinear')\n",
    "tree = DecisionTreeClassifier()\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "features = train.drop('Movement type').rows # predictor\n",
    "label = train.column('Movement type') == 'horizontal straight-line' # outcome\n",
    "\n",
    "logit.fit(X = features, y = label)\n",
    "tree.fit(X = features, y = label)\n",
    "knn.fit(X = features, y = label)\n",
    "\n",
    "for i in make_array(.1,.3,.5,.7,.9):\n",
    "    knn_predicted = knn.predict_proba(test_features)[:,1] > i\n",
    "    logit_predicted = logit.predict_proba(test_features)[:,1] > i\n",
    "    tree_predicted = tree.predict_proba(test_features)[:,1] > i\n",
    "\n",
    "    knn_precision = np.append(knn_precision, precision_score(expected,knn_predicted))\n",
    "    logit_precision = np.append(logit_precision, precision_score(expected,logit_predicted))\n",
    "    tree_precision = np.append(tree_precision, precision_score(expected,tree_predicted))\n",
    "    \n",
    "    knn_recall = np.append(knn_recall, recall_score(expected,knn_predicted))\n",
    "    logit_recall = np.append(logit_recall, recall_score(expected,logit_predicted))\n",
    "    tree_recall = np.append(tree_recall, recall_score(expected,tree_predicted))\n",
    "    \n",
    "# You can use these to look at results\n",
    "precision_results = Table().with_columns('Threshold', make_array(.1, .3, .5, .7, .9),\n",
    "                              'KNN Precision', knn_precision,\n",
    "                              'Logistic Regression Precision', logit_precision,\n",
    "                              'Decision Tree Precision', tree_precision)\n",
    "\n",
    "recall_results = Table().with_columns('Threshold', make_array(.1, .3, .5, .7, .9),\n",
    "                              'KNN Recall', knn_recall,\n",
    "                              'Logistic Regression Recall', logit_recall,\n",
    "                              'Decision Tree Recall', tree_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Threshold</th> <th>KNN Precision</th> <th>Logistic Regression Precision</th> <th>Decision Tree Precision</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0.1      </td> <td>0.944954     </td> <td>0.895652                     </td> <td>0.94                   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.3      </td> <td>0.944954     </td> <td>0.953704                     </td> <td>0.94                   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.5      </td> <td>0.980769     </td> <td>1                            </td> <td>0.94                   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.7      </td> <td>0.99         </td> <td>1                            </td> <td>0.94                   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.9      </td> <td>0.99         </td> <td>1                            </td> <td>0.94                   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Threshold | KNN Precision | Logistic Regression Precision | Decision Tree Precision\n",
       "0.1       | 0.944954      | 0.895652                      | 0.94\n",
       "0.3       | 0.944954      | 0.953704                      | 0.94\n",
       "0.5       | 0.980769      | 1                             | 0.94\n",
       "0.7       | 0.99          | 1                             | 0.94\n",
       "0.9       | 0.99          | 1                             | 0.94"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Threshold</th> <th>KNN Recall</th> <th>Logistic Regression Recall</th> <th>Decision Tree Recall</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0.1      </td> <td>1         </td> <td>1                         </td> <td>0.912621            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.3      </td> <td>1         </td> <td>1                         </td> <td>0.912621            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.5      </td> <td>0.990291  </td> <td>1                         </td> <td>0.912621            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.7      </td> <td>0.961165  </td> <td>0.980583                  </td> <td>0.912621            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.9      </td> <td>0.961165  </td> <td>0.902913                  </td> <td>0.912621            </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Threshold | KNN Recall | Logistic Regression Recall | Decision Tree Recall\n",
       "0.1       | 1          | 1                          | 0.912621\n",
       "0.3       | 1          | 1                          | 0.912621\n",
       "0.5       | 0.990291   | 1                          | 0.912621\n",
       "0.7       | 0.961165   | 0.980583                   | 0.912621\n",
       "0.9       | 0.961165   | 0.902913                   | 0.912621"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Model Selection and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, when deciding on the best model, we compare the models we fit with each other, as well as against a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 6. Suppose we were to choose the .5 threshold. If we were to try to predict whether a hand movement was \"horizontal straight-line\" or not completely randomly, how often would we be right? That is, what would be our precision if we were guessing completely randomly? How well does our best model perform compared to that baseline?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
